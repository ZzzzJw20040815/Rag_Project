"""
å­¦æœ¯æ–‡çŒ®æ™ºèƒ½å¯¼è¯»ä¸å¯è§†åŒ–åˆ†æç³»ç»Ÿ
Academic Literature Intelligent Guidance System

ä¸»åº”ç”¨å…¥å£
"""

import streamlit as st
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import get_api_key
from core.document_processor import DocumentProcessor
from core.vector_store import VectorStoreManager
from core.rag_chain import RAGChain
from core.entity_extractor import EntityExtractor
from core.knowledge_graph import KnowledgeGraph
from ui import (
    init_page_config,
    get_custom_css,
    render_chat_message,
    render_source_documents,
    render_sidebar_api_config,
    render_sidebar_info,
    render_quick_questions
)
from ui.graph_view import (
    render_graph_in_streamlit,
    render_graph_statistics,
    render_legend
)


def init_session_state():
    """åˆå§‹åŒ– session state"""
    if "vector_store_manager" not in st.session_state:
        st.session_state.vector_store_manager = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "documents_loaded" not in st.session_state:
        st.session_state.documents_loaded = False
    if "uploaded_files_info" not in st.session_state:
        st.session_state.uploaded_files_info = []
    if "api_key" not in st.session_state:
        st.session_state.api_key = get_api_key()
    if "current_question" not in st.session_state:
        st.session_state.current_question = ""
    # çŸ¥è¯†å›¾è°±ç›¸å…³çŠ¶æ€
    if "knowledge_graph" not in st.session_state:
        st.session_state.knowledge_graph = KnowledgeGraph()
    if "entities_extracted" not in st.session_state:
        st.session_state.entities_extracted = False
    if "processed_chunks" not in st.session_state:
        st.session_state.processed_chunks = []


def process_uploaded_files(uploaded_files, api_key: str):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    
    Args:
        uploaded_files: Streamlit ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        api_key: API Key
    """
    if not uploaded_files:
        return
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    doc_processor = DocumentProcessor()
    
    # åˆå§‹åŒ–æˆ–è·å–å‘é‡å­˜å‚¨ç®¡ç†å™¨
    if st.session_state.vector_store_manager is None:
        st.session_state.vector_store_manager = VectorStoreManager(api_key=api_key)
    
    all_chunks = []
    files_info = []
    
    for uploaded_file in uploaded_files:
        filename = uploaded_file.name
        
        with st.spinner(f"ğŸ“– æ­£åœ¨å¤„ç† {filename}..."):
            try:
                # å¤„ç†æ–‡ä»¶ï¼šè§£æ + åˆ‡åˆ†
                chunks = doc_processor.process_uploaded_file(
                    uploaded_file,
                    filename,
                    clean=True  # æ¸…æ´—æ–‡æœ¬ï¼Œç§»é™¤å‚è€ƒæ–‡çŒ®ç­‰
                )
                
                all_chunks.extend(chunks)
                files_info.append({
                    "name": filename,
                    "size": uploaded_file.size,
                    "chunks": len(chunks)
                })
                
                st.success(f"âœ… {filename}: è§£æå®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                st.error(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {str(e)}")
    
    if all_chunks:
        with st.spinner("ğŸ”¢ æ­£åœ¨åˆ›å»ºå‘é‡ç´¢å¼•..."):
            try:
                # åˆ›å»ºå‘é‡å­˜å‚¨
                st.session_state.vector_store_manager.create_from_documents(
                    all_chunks,
                    persist=True
                )
                
                st.session_state.documents_loaded = True
                st.session_state.uploaded_files_info = files_info
                # ä¿å­˜ chunks ä¾›çŸ¥è¯†å›¾è°±ä½¿ç”¨
                st.session_state.processed_chunks = all_chunks
                # é‡ç½®çŸ¥è¯†å›¾è°±çŠ¶æ€ï¼ˆéœ€è¦é‡æ–°æå–å®ä½“ï¼‰
                st.session_state.knowledge_graph = KnowledgeGraph()
                st.session_state.entities_extracted = False
                
                total_chunks = sum(f["chunks"] for f in files_info)
                st.success(f"âœ… å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸï¼å…±å¤„ç† {len(files_info)} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                st.error(f"âŒ åˆ›å»ºå‘é‡ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")


def handle_question(question: str, api_key: str):
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        api_key: API Key
    """
    if not question.strip():
        return
    
    if not st.session_state.documents_loaded:
        st.warning("è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
        return
    
    with st.spinner("ğŸ¤” æ­£åœ¨æ€è€ƒ..."):
        try:
            # è·å–æ£€ç´¢å™¨
            retriever = st.session_state.vector_store_manager.as_retriever()
            
            # åˆ›å»º RAG é“¾
            rag_chain = RAGChain(retriever, api_key=api_key)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = rag_chain.query(question)
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            st.session_state.chat_history.append({
                "question": question,
                "answer": result["answer"],
                "sources": result["sources"]
            })
            
            # æ¸…ç©ºè¾“å…¥æ¡†
            st.session_state.current_question = ""
            
            # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ å›ç­”é—®é¢˜æ—¶å‡ºé”™: {str(e)}")


def render_document_status():
    """æ¸²æŸ“æ–‡æ¡£çŠ¶æ€ä¿¡æ¯"""
    if st.session_state.documents_loaded:
        st.success("âœ… æ–‡æ¡£å·²åŠ è½½ï¼Œå¯ä»¥å¼€å§‹æé—®ï¼")
        
        # æ˜¾ç¤ºå·²ä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯
        if st.session_state.uploaded_files_info:
            with st.expander("ğŸ“‹ å·²åŠ è½½çš„æ–‡æ¡£", expanded=False):
                for f in st.session_state.uploaded_files_info:
                    st.markdown(f"- **{f['name']}** ({f['size']/1024:.1f} KB, {f['chunks']} ä¸ªæ–‡æœ¬å—)")
        
        # æ¸…é™¤æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ–‡æ¡£", use_container_width=True):
            if st.session_state.vector_store_manager:
                st.session_state.vector_store_manager.clear()
            st.session_state.vector_store_manager = None
            st.session_state.documents_loaded = False
            st.session_state.uploaded_files_info = []
            st.session_state.chat_history = []
            # æ¸…é™¤çŸ¥è¯†å›¾è°±ç›¸å…³çŠ¶æ€
            st.session_state.knowledge_graph = KnowledgeGraph()
            st.session_state.entities_extracted = False
            st.session_state.processed_chunks = []
            st.rerun()
    else:
        st.info("â³ è¯·ä¸Šä¼ æ–‡æ¡£å¼€å§‹ä½¿ç”¨")


def render_chat_interface(api_key: str):
    """
    æ¸²æŸ“èŠå¤©ç•Œé¢
    
    Args:
        api_key: API Key
    """
    st.subheader("ğŸ’¬ æ™ºèƒ½é—®ç­”")
    
    # æ¸²æŸ“å†å²æ¶ˆæ¯
    for chat in st.session_state.chat_history:
        render_chat_message("user", chat["question"])
        render_chat_message("assistant", chat["answer"])
        render_source_documents(chat.get("sources", []))
    
    # é—®ç­”è¾“å…¥åŒº
    if st.session_state.documents_loaded and api_key:
        # å¿«æ·é—®é¢˜
        quick_q = render_quick_questions()
        if quick_q:
            st.session_state.current_question = quick_q
            # å¼ºåˆ¶åˆ·æ–°ä»¥æ›´æ–°è¾“å…¥æ¡†
            st.rerun()
        
        st.markdown("---")
        
        # è¾“å…¥æ¡†
        # ä½¿ç”¨ callback ä¼šæ›´å¥½ï¼Œä½†è¿™é‡Œç®€å•èµ·è§ï¼Œåˆ©ç”¨ session_state ç»‘å®š
        if "current_question" not in st.session_state:
            st.session_state.current_question = ""
            
        question = st.text_input(
            "è¾“å…¥ä½ çš„é—®é¢˜",
            value=st.session_state.current_question,
            placeholder="è¯·è¾“å…¥å…³äºæ–‡æ¡£çš„é—®é¢˜...",
            key="question_input"
        )
        
        # è¾“å…¥æ¡†çš„å€¼å˜åŒ–æ—¶ï¼Œå¯èƒ½ä¼šæ›´æ–° key å¯¹åº”çš„ stateï¼Œä½†ä¸ä¼šè‡ªåŠ¨åŒæ­¥åˆ° current_question
        # æ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠ input çš„å€¼å›å†™åˆ° logic state (å¦‚æœéœ€è¦çš„è¯)
        # ä½†è¿™é‡Œä¸»è¦å°±æ˜¯ä¸ºäº†è®© quick_q ç‚¹å‡»åå¡«å……è¿›å»ã€‚
        
        col1, col2 = st.columns([1, 5])
        with col1:
            if st.button("ğŸ” æé—®", use_container_width=True):
                handle_question(question, api_key)
        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", use_container_width=True):
                st.session_state.chat_history = []
                st.session_state.current_question = ""
                st.rerun()
                
    elif not st.session_state.documents_loaded:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
    elif not api_key:
        st.warning("ğŸ‘ˆ è¯·åœ¨ä¾§è¾¹æ é…ç½® API Key")


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–é¡µé¢é…ç½®
    init_page_config()
    
    # åˆå§‹åŒ– session state
    init_session_state()
    
    # æ¸²æŸ“ä¾§è¾¹æ 
    api_key = render_sidebar_api_config()
    st.session_state.api_key = api_key
    render_sidebar_info()
    
    # ä¸»æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸ“š å­¦æœ¯æ–‡çŒ®æ™ºèƒ½å¯¼è¯»ç³»ç»Ÿ</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">åŸºäºçŸ¥è¯†å›¾è°±å¢å¼ºçš„ RAG æ–‡æ¡£é—®ç­”ä¸å¯è§†åŒ–åˆ†æ</p>', unsafe_allow_html=True)
    
    # ä½¿ç”¨ Tab ç»„ç»‡ç•Œé¢
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ æ–‡æ¡£ä¸Šä¼ ", "ğŸ’¬ æ™ºèƒ½é—®ç­”", "ğŸŒ çŸ¥è¯†å›¾è°±"])
    
    with tab1:
        st.subheader("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # æ–‡ä»¶ä¸Šä¼ 
            uploaded_files = st.file_uploader(
                "é€‰æ‹© PDF æˆ– Word æ–‡ä»¶",
                type=["pdf", "docx", "doc"],
                accept_multiple_files=True,
                help="æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶"
            )
            
            if uploaded_files:
                st.write(f"ğŸ“„ å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
                for f in uploaded_files:
                    st.write(f"  - {f.name} ({f.size/1024:.1f} KB)")
                
                if st.button("ğŸš€ å¤„ç†æ–‡æ¡£", use_container_width=True):
                    if not api_key:
                        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                    else:
                        process_uploaded_files(uploaded_files, api_key)
        
        with col2:
            st.subheader("ğŸ“Š çŠ¶æ€")
            render_document_status()
    
    with tab2:
        render_chat_interface(api_key)
    
    with tab3:
        st.subheader("ğŸ—ºï¸ è®ºæ–‡åœ°å›¾ - çŸ¥è¯†å›¾è°±")
        
        if not st.session_state.documents_loaded:
            st.info("ğŸ‘† è¯·å…ˆåœ¨ã€Œæ–‡æ¡£ä¸Šä¼ ã€é¡µé¢ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
        else:
            # å®ä½“æå–æŒ‰é’®
            col1, col2 = st.columns([1, 3])
            with col1:
                extract_btn = st.button(
                    "ğŸ” æå–å®ä½“" if not st.session_state.entities_extracted else "ğŸ”„ é‡æ–°æå–",
                    use_container_width=True
                )
            with col2:
                if st.session_state.entities_extracted:
                    st.success("âœ… å®ä½“å·²æå–ï¼Œå›¾è°±å·²ç”Ÿæˆ")
                else:
                    st.info("ç‚¹å‡»ã€Œæå–å®ä½“ã€æŒ‰é’®å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±")
            
            # æ‰§è¡Œå®ä½“æå–
            if extract_btn and api_key:
                with st.spinner("ğŸ” æ­£åœ¨ä½¿ç”¨ AI æå–æ–‡æ¡£å®ä½“..."):
                    try:
                        # åˆ›å»ºå®ä½“æå–å™¨
                        extractor = EntityExtractor(api_key=api_key)
                        
                        # ä» chunks ä¸­æå–å®ä½“
                        extraction_results = extractor.extract_from_documents(
                            st.session_state.processed_chunks
                        )
                        
                        # æ„å»ºçŸ¥è¯†å›¾è°±
                        st.session_state.knowledge_graph = KnowledgeGraph()
                        st.session_state.knowledge_graph.build_from_extraction_results(
                            extraction_results
                        )
                        
                        # ä¿å­˜å›¾è°±
                        st.session_state.knowledge_graph.save()
                        st.session_state.entities_extracted = True
                        
                        st.success(f"âœ… æˆåŠŸæå– {len(extraction_results)} ä¸ªæ–‡æ¡£çš„å®ä½“ï¼")
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ å®ä½“æå–å¤±è´¥: {str(e)}")
            
            # æ˜¾ç¤ºçŸ¥è¯†å›¾è°±
            if st.session_state.entities_extracted:
                st.markdown("---")
                
                # å›¾ä¾‹
                render_legend()
                
                # å›¾è°±ç»Ÿè®¡
                stats = st.session_state.knowledge_graph.get_statistics()
                render_graph_statistics(stats)
                
                st.markdown("---")
                st.markdown("### ğŸ“Š äº¤äº’å¼è®ºæ–‡åœ°å›¾")
                st.caption("æç¤ºï¼šå¯æ‹–æ‹½èŠ‚ç‚¹ï¼Œæ‚¬åœæŸ¥çœ‹è¯¦æƒ…")
                
                # æ¸²æŸ“å›¾è°±
                render_graph_in_streamlit(
                    st.session_state.knowledge_graph.graph,
                    height=550,
                    key="main_knowledge_graph"
                )
    
    # åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    st.markdown(
        '<p class="footer">ğŸ’¡ åŸºäº LangChain + ChromaDB + Streamlit æ„å»ºçš„å­¦æœ¯æ–‡çŒ®æ™ºèƒ½åˆ†æç³»ç»Ÿ</p>',
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()

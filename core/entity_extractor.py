"""
å®ä½“æå–æ¨¡å— (æé€Ÿä¼˜åŒ–ç‰ˆ v2)
ä¼˜åŒ–ç­–ç•¥ï¼š
1. åˆå¹¶ç‰‡æ®µï¼šå°†å¤šä¸ªæ–‡æœ¬ç‰‡æ®µåˆå¹¶åä¸€æ¬¡æ€§å‘é€ç»™ LLMï¼Œå‡å°‘ API è°ƒç”¨æ¬¡æ•°
2. å¹¶è¡Œè°ƒç”¨ï¼šä½¿ç”¨ LangChain batch() æ–¹æ³•å¹¶å‘è°ƒç”¨ API
3. åŠ¨æ€é‡‡æ ·ï¼šé’ˆå¯¹é•¿æ–‡æ¡£è‡ªåŠ¨ç¨€ç–é‡‡æ ·
4. ä¿æŒåŒè¯­è¾“å‡º
"""

import json
import re
import time
from typing import Dict, List, Optional, Any
from collections import Counter
from langchain_openai import ChatOpenAI
from langchain.schema import Document

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from config import (
    SILICONFLOW_BASE_URL,
    LLM_MODEL,
    MAX_KEYWORDS_PER_DOC,
    MAX_METHODS_PER_DOC,
    MAX_DATASETS_PER_DOC,
    get_api_key
)

# é’ˆå¯¹åˆå¹¶ç‰‡æ®µçš„ä¼˜åŒ– Prompt
ENTITY_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡çŒ®ç‰‡æ®µä¸­æå–æ ¸å¿ƒå®ä½“ã€‚

ã€æ–‡æœ¬ç‰‡æ®µã€‘
{text}

ã€æå–è¦æ±‚ã€‘
è¯·æå–ä»¥ä¸‹ 5 ç±»å®ä½“ã€‚**é‡è¦ï¼šå¦‚æœå®ä½“æ˜¯è‹±æ–‡ï¼Œè¯·åŠ¡å¿…åœ¨æ‹¬å·å†…é™„ä¸Šä¸­æ–‡ç¿»è¯‘**ï¼Œæ ¼å¼ä¸º `English Term (ä¸­æ–‡ç¿»è¯‘)`ã€‚

1. **Keywords** (å…³é”®è¯): ç ”ç©¶çš„æ ¸å¿ƒä¸»é¢˜ (æå– {max_keywords} ä¸ª)
2. **Methods** (æ–¹æ³•): ç®—æ³•ã€æ¨¡å‹ (æå– {max_methods} ä¸ª)
3. **Fields** (é¢†åŸŸ): ç ”ç©¶é¢†åŸŸ (æå– {max_fields} ä¸ª)
4. **Datasets** (æ•°æ®é›†): æ•°æ®é›†åç§° (æå– {max_datasets} ä¸ª)
5. **Applications** (åº”ç”¨): åº”ç”¨åœºæ™¯ (æå– {max_applications} ä¸ª)

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSON æ ¼å¼ï¼š
{{
  "keywords": ["Term A (ç¿»è¯‘A)", "Term B (ç¿»è¯‘B)"],
  "methods": [],
  "fields": [],
  "datasets": [],
  "applications": []
}}
"""

# ============================================
# ä¼˜åŒ–é…ç½®å‚æ•°
# ============================================
CHUNKS_PER_BATCH = 4        # æ¯æ‰¹åˆå¹¶çš„ç‰‡æ®µæ•°é‡
MAX_CONCURRENT_REQUESTS = 3  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆé¿å…è§¦å‘ RPM é™åˆ¶ï¼‰
TARGET_BATCHES = 6          # ç›®æ ‡æ‰¹æ¬¡æ•°ï¼ˆåŸæ¥ 25 æ¬¡è°ƒç”¨ -> 6 æ‰¹å¹¶å‘ï¼‰


class EntityExtractor:
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        max_keywords: int = MAX_KEYWORDS_PER_DOC,
        max_methods: int = MAX_METHODS_PER_DOC,
        max_datasets: int = MAX_DATASETS_PER_DOC
    ):
        self.api_key = api_key or get_api_key()
        self.max_keywords = max_keywords
        self.max_methods = max_methods
        self.max_datasets = max_datasets
        self.max_fields = 4
        self.max_applications = 4
        self._llm = None
    
    @property
    def llm(self) -> ChatOpenAI:
        if self._llm is None:
            if not self.api_key:
                raise ValueError("è¯·å…ˆé…ç½® API Keyï¼")
            self._llm = ChatOpenAI(
                model=LLM_MODEL,
                openai_api_key=self.api_key,
                openai_api_base=SILICONFLOW_BASE_URL,
                temperature=0.3,
                max_tokens=2048
            )
        return self._llm
    
    def _parse_llm_response(self, response: str) -> Dict[str, List[str]]:
        """è§£æ LLM è¿”å›çš„ JSON å“åº”"""
        default_result = {k: [] for k in ["keywords", "methods", "fields", "datasets", "applications"]}
        try:
            cleaned_response = response.replace("```json", "").replace("```", "").strip()
            result = json.loads(cleaned_response)
        except json.JSONDecodeError:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                except:
                    return default_result
            else:
                return default_result
        
        final_result = {}
        for key in default_result.keys():
            items = result.get(key, [])
            if isinstance(items, list):
                final_result[key] = list(set([str(i).strip() for i in items if i]))
            else:
                final_result[key] = []
        return final_result

    def _merge_chunks(self, chunks: List[str], max_chars_per_chunk: int = 1500) -> str:
        """
        åˆå¹¶å¤šä¸ªæ–‡æœ¬ç‰‡æ®µä¸ºä¸€ä¸ªè¶…çº§ç‰‡æ®µ
        æ¯ä¸ªç‰‡æ®µæˆªå–å‰ max_chars_per_chunk ä¸ªå­—ç¬¦ï¼Œç”¨åˆ†éš”ç¬¦è¿æ¥
        """
        merged_parts = []
        for i, chunk in enumerate(chunks):
            truncated = chunk[:max_chars_per_chunk].strip()
            if truncated:
                merged_parts.append(f"[ç‰‡æ®µ {i+1}]\n{truncated}")
        return "\n\n---\n\n".join(merged_parts)

    def _select_representative_chunks(self, docs: List[Document]) -> List[List[str]]:
        """
        æ™ºèƒ½é€‰æ‹©ä»£è¡¨æ€§ç‰‡æ®µå¹¶åˆ†ç»„
        è¿”å›ï¼šåˆ†ç»„åçš„ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ç»„ CHUNKS_PER_BATCH ä¸ªç‰‡æ®µ
        """
        total_chunks = len(docs)
        
        # ç›®æ ‡ï¼šé€‰å– TARGET_BATCHES * CHUNKS_PER_BATCH ä¸ªç‰‡æ®µ
        target_samples = TARGET_BATCHES * CHUNKS_PER_BATCH  # 6 * 4 = 24 ä¸ªç‰‡æ®µ
        
        # å§‹ç»ˆåŒ…å«å¼€å¤´çš„å‡ ä¸ªç‰‡æ®µï¼ˆé€šå¸¸åŒ…å«æ‘˜è¦å’Œä»‹ç»ï¼‰
        selected_indices = [0, 1, 2]
        
        if total_chunks > 3:
            # åŠ¨æ€æ­¥é•¿é‡‡æ ·
            remaining_samples = target_samples - 3
            step = max(1, (total_chunks - 3) // remaining_samples)
            selected_indices.extend(range(3, total_chunks, step))
        
        # é™åˆ¶æœ€å¤§é‡‡æ ·æ•°
        selected_indices = selected_indices[:target_samples]
        
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„ç‰‡æ®µï¼Œå¹¶æå–æ–‡æœ¬
        valid_chunks = []
        for idx in selected_indices:
            if idx < total_chunks:
                text = docs[idx].page_content
                if len(text) >= 100:  # è¿‡æ»¤å¤ªçŸ­çš„ç‰‡æ®µ
                    valid_chunks.append(text)
        
        # åˆ†ç»„ï¼šæ¯ CHUNKS_PER_BATCH ä¸ªç‰‡æ®µä¸ºä¸€ç»„
        batches = []
        for i in range(0, len(valid_chunks), CHUNKS_PER_BATCH):
            batch = valid_chunks[i:i + CHUNKS_PER_BATCH]
            if batch:
                batches.append(batch)
        
        return batches

    def extract_from_documents(
        self,
        documents: List[Document],
        aggregate_by_file: bool = True
    ) -> Dict[str, Dict[str, List[str]]]:
        """
        ä»æ–‡æ¡£åˆ—è¡¨ä¸­æå–å®ä½“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. åˆå¹¶ç‰‡æ®µï¼šæ¯ 4 ä¸ªç‰‡æ®µåˆå¹¶ä¸º 1 ä¸ªè¶…çº§ç‰‡æ®µ
        2. å¹¶è¡Œè°ƒç”¨ï¼šä½¿ç”¨ batch() å¹¶å‘å‘é€è¯·æ±‚
        """
        if not documents:
            return {}
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„
        file_docs = {}
        for doc in documents:
            source_file = doc.metadata.get("source_file", "æœªçŸ¥æ–‡ä»¶")
            if source_file not in file_docs:
                file_docs[source_file] = []
            file_docs[source_file].append(doc)
        
        results = {}
        
        for source_file, docs in file_docs.items():
            start_time = time.time()
            total_chunks = len(docs)
            
            # è·å–åˆ†ç»„åçš„ç‰‡æ®µ
            chunk_batches = self._select_representative_chunks(docs)
            
            print(f"ğŸ“„ åˆ†æ: {source_file}")
            print(f"   ğŸ“Š æ€»é¡µæ•°: {total_chunks} | é‡‡æ ·ç‰‡æ®µ: {sum(len(b) for b in chunk_batches)} | åˆå¹¶ä¸º {len(chunk_batches)} æ‰¹")
            
            # æ„å»ºæ‰€æœ‰ prompts
            all_prompts = []
            for batch in chunk_batches:
                merged_text = self._merge_chunks(batch)
                prompt = ENTITY_EXTRACTION_PROMPT.format(
                    text=merged_text,
                    max_keywords=8,  # åˆå¹¶ç‰‡æ®µåå¯ä»¥å¤šæå–ä¸€äº›
                    max_methods=6,
                    max_fields=4,
                    max_datasets=4,
                    max_applications=4
                )
                all_prompts.append(prompt)
            
            # å®ä½“èšåˆå™¨
            aggregator = {k: Counter() for k in ["keywords", "methods", "fields", "datasets", "applications"]}
            
            # åˆ†æ‰¹å¹¶è¡Œè°ƒç”¨ï¼ˆæ¯æ‰¹æœ€å¤š MAX_CONCURRENT_REQUESTS ä¸ªè¯·æ±‚ï¼‰
            for i in range(0, len(all_prompts), MAX_CONCURRENT_REQUESTS):
                batch_prompts = all_prompts[i:i + MAX_CONCURRENT_REQUESTS]
                batch_num = i // MAX_CONCURRENT_REQUESTS + 1
                total_batches = (len(all_prompts) + MAX_CONCURRENT_REQUESTS - 1) // MAX_CONCURRENT_REQUESTS
                
                print(f"   ğŸš€ å¹¶è¡Œè¯·æ±‚æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_prompts)} ä¸ªè¯·æ±‚)...")
                
                try:
                    # ä½¿ç”¨ LangChain çš„ batch() æ–¹æ³•å¹¶å‘è°ƒç”¨
                    responses = self.llm.batch(batch_prompts)
                    
                    for response in responses:
                        chunk_result = self._parse_llm_response(response.content)
                        for key in aggregator:
                            aggregator[key].update(chunk_result.get(key, []))
                            
                except Exception as e:
                    print(f"   âš ï¸ æ‰¹æ¬¡ {batch_num} éƒ¨åˆ†å¤±è´¥: {str(e)[:50]}")
                    # é™çº§ï¼šé€ä¸ªè¯·æ±‚
                    for prompt in batch_prompts:
                        try:
                            response = self.llm.invoke(prompt)
                            chunk_result = self._parse_llm_response(response.content)
                            for key in aggregator:
                                aggregator[key].update(chunk_result.get(key, []))
                        except:
                            pass
            
            # æ±‡æ€»æœ€ç»ˆç»“æœ
            final_entities = {}
            for key, counter in aggregator.items():
                limit = self.max_keywords * 2 if key == "keywords" else self.max_methods * 2
                most_common = [item for item, count in counter.most_common(limit)]
                final_entities[key] = most_common
            
            elapsed = time.time() - start_time
            results[source_file] = final_entities
            
            # ç»Ÿè®¡ä¿¡æ¯
            entity_count = sum(len(v) for v in final_entities.values())
            print(f"   âœ… å®Œæˆï¼è€—æ—¶ {elapsed:.1f}s | æå– {entity_count} ä¸ªå®ä½“")
        
        return results


def create_entity_extractor(api_key: Optional[str] = None) -> EntityExtractor:
    """åˆ›å»ºå®ä½“æå–å™¨å®ä¾‹"""
    return EntityExtractor(api_key=api_key)
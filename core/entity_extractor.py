"""
å®ä½“æå–æ¨¡å—
ä½¿ç”¨ LLM ä»å­¦æœ¯æ–‡çŒ®ä¸­æå–å…³é”®è¯ã€æ–¹æ³•å’Œæ•°æ®é›†ç­‰å®ä½“

ä¸»è¦åŠŸèƒ½ï¼š
- ä»æ–‡æ¡£æ–‡æœ¬ä¸­æå–æ ¸å¿ƒå®ä½“
- æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£
- è¾“å‡ºç»“æ„åŒ–çš„å®ä½“æ•°æ®
"""

import json
import re
from typing import Dict, List, Optional, Any
from langchain_openai import ChatOpenAI
from langchain.schema import Document

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from config import (
    SILICONFLOW_BASE_URL,
    LLM_MODEL,
    LLM_TEMPERATURE,
    MAX_KEYWORDS_PER_DOC,
    MAX_METHODS_PER_DOC,
    MAX_DATASETS_PER_DOC,
    get_api_key
)


# å®ä½“æå–çš„ Prompt æ¨¡æ¿ - å¢å¼ºç‰ˆï¼Œæå–æ›´ä¸°å¯Œçš„å®ä½“ç±»å‹
ENTITY_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬ä¸­å…¨é¢æå–æ ¸å¿ƒå®ä½“ï¼Œæ„å»ºä¸°å¯Œçš„çŸ¥è¯†ç½‘ç»œã€‚

ã€æ–‡æœ¬å†…å®¹ã€‘
{text}

ã€æå–è¦æ±‚ã€‘
è¯·å°½å¯èƒ½å…¨é¢åœ°æå–ä»¥ä¸‹ç±»å‹çš„å®ä½“ï¼š

1. **å…³é”®è¯** (Keywords): æå– {max_keywords} ä¸ªæ ¸å¿ƒæ¦‚å¿µã€æœ¯è¯­æˆ–ç ”ç©¶ä¸»é¢˜
   - åŒ…æ‹¬ï¼šç ”ç©¶å¯¹è±¡ã€æ ¸å¿ƒé—®é¢˜ã€åˆ›æ–°ç‚¹ç­‰
   
2. **æ–¹æ³•/æŠ€æœ¯** (Methods): æå– {max_methods} ä¸ªæŠ€æœ¯æ–¹æ³•
   - åŒ…æ‹¬ï¼šç®—æ³•ã€æ¡†æ¶ã€æ¨¡å‹ã€å·¥å…·ã€æŠ€æœ¯æ‰‹æ®µç­‰
   - ä¾‹å¦‚ï¼šTransformerã€BERTã€RAGã€çŸ¥è¯†å›¾è°±ã€å‘é‡æ£€ç´¢ç­‰

3. **ç ”ç©¶é¢†åŸŸ** (Fields): æå– {max_fields} ä¸ªç›¸å…³ç ”ç©¶é¢†åŸŸ
   - åŒ…æ‹¬ï¼šå­¦ç§‘æ–¹å‘ã€ç ”ç©¶åˆ†æ”¯ã€äº¤å‰é¢†åŸŸç­‰
   - ä¾‹å¦‚ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ã€æœºå™¨å­¦ä¹ ã€ä¿¡æ¯æ£€ç´¢ç­‰

4. **æ•°æ®é›†** (Datasets): æå– {max_datasets} ä¸ªæ•°æ®é›†åç§°
   - å¦‚æœæ²¡æœ‰æ˜ç¡®æåˆ°ï¼Œå¯ä»¥æ˜¯ç©ºæ•°ç»„

5. **åº”ç”¨åœºæ™¯** (Applications): æå– {max_applications} ä¸ªåº”ç”¨åœºæ™¯
   - åŒ…æ‹¬ï¼šå®é™…ç”¨é€”ã€åº”ç”¨è¡Œä¸šã€è§£å†³çš„é—®é¢˜ç­‰
   - ä¾‹å¦‚ï¼šé—®ç­”ç³»ç»Ÿã€æ–‡æ¡£æ£€ç´¢ã€æ™ºèƒ½å®¢æœç­‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•æ–‡å­—ï¼š
{{
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...],
  "methods": ["æ–¹æ³•1", "æ–¹æ³•2", ...],
  "fields": ["é¢†åŸŸ1", "é¢†åŸŸ2", ...],
  "datasets": ["æ•°æ®é›†1", ...],
  "applications": ["åº”ç”¨1", "åº”ç”¨2", ...]
}}

ã€æ³¨æ„äº‹é¡¹ã€‘
- å°½é‡æå–å…·ä½“ã€æœ‰åŒºåˆ†åº¦çš„å®ä½“ï¼Œé¿å…è¿‡äºå®½æ³›
- æ¯ä¸ªç±»åˆ«å°½é‡æå–åˆ°ä¸Šé™æ•°é‡ï¼Œä»¥æ„å»ºä¸°å¯Œçš„çŸ¥è¯†ç½‘ç»œ
- ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼"""


class EntityExtractor:
    """
    å®ä½“æå–å™¨
    ä»æ–‡æ¡£ä¸­æå–å…³é”®è¯ã€æ–¹æ³•å’Œæ•°æ®é›†
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        max_keywords: int = MAX_KEYWORDS_PER_DOC,
        max_methods: int = MAX_METHODS_PER_DOC,
        max_datasets: int = MAX_DATASETS_PER_DOC
    ):
        """
        åˆå§‹åŒ–å®ä½“æå–å™¨
        
        Args:
            api_key: å¯é€‰çš„ API Key
            max_keywords: æ¯ç¯‡æ–‡æ¡£æå–çš„æœ€å¤§å…³é”®è¯æ•°
            max_methods: æ¯ç¯‡æ–‡æ¡£æå–çš„æœ€å¤§æ–¹æ³•æ•°
            max_datasets: æ¯ç¯‡æ–‡æ¡£æå–çš„æœ€å¤§æ•°æ®é›†æ•°
        """
        self.api_key = api_key or get_api_key()
        self.max_keywords = max_keywords
        self.max_methods = max_methods
        self.max_datasets = max_datasets
        # æ–°å¢å®ä½“ç±»å‹çš„æ•°é‡é…ç½®
        self.max_fields = 4  # ç ”ç©¶é¢†åŸŸ
        self.max_applications = 3  # åº”ç”¨åœºæ™¯
        self._llm = None
    
    @property
    def llm(self) -> ChatOpenAI:
        """æ‡’åŠ è½½ LLM"""
        if self._llm is None:
            if not self.api_key:
                raise ValueError("è¯·å…ˆé…ç½® API Keyï¼")
            
            self._llm = ChatOpenAI(
                model=LLM_MODEL,
                openai_api_key=self.api_key,
                openai_api_base=SILICONFLOW_BASE_URL,
                temperature=0.3,  # å®ä½“æå–ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
                max_tokens=1024
            )
        return self._llm
    
    def _parse_llm_response(self, response: str) -> Dict[str, List[str]]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„å®ä½“å­—å…¸
        """
        # é»˜è®¤è¿”å›ç»“æ„
        default_result = {
            "keywords": [],
            "methods": [],
            "datasets": []
        }
        
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            result = json.loads(response)
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå– JSON éƒ¨åˆ†
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                except json.JSONDecodeError:
                    print(f"âš ï¸ JSON è§£æå¤±è´¥: {response[:100]}...")
                    return default_result
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆ JSON: {response[:100]}...")
                return default_result
        
        # éªŒè¯å¹¶æ¸…ç†ç»“æœ - æ”¯æŒæ›´å¤šå®ä½“ç±»å‹
        cleaned = {}
        for key in ["keywords", "methods", "fields", "datasets", "applications"]:
            if key in result and isinstance(result[key], list):
                # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œéå­—ç¬¦ä¸²å…ƒç´ 
                cleaned[key] = [
                    str(item).strip() 
                    for item in result[key] 
                    if item and str(item).strip()
                ]
            else:
                cleaned[key] = []
        
        return cleaned
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """
        ä»å•æ®µæ–‡æœ¬ä¸­æå–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯è®ºæ–‡æ‘˜è¦æˆ–é¦–é¡µå†…å®¹ï¼‰
            
        Returns:
            åŒ…å« keywords, methods, datasets çš„å­—å…¸
        """
        if not text or len(text.strip()) < 50:
            return {"keywords": [], "methods": [], "datasets": []}
        
        # æˆªå–åˆé€‚é•¿åº¦çš„æ–‡æœ¬ï¼ˆé¿å…è¶…é•¿è¾“å…¥ï¼‰
        max_text_length = 3000
        if len(text) > max_text_length:
            text = text[:max_text_length] + "..."
        
        # æ„å»º prompt
        prompt = ENTITY_EXTRACTION_PROMPT.format(
            text=text,
            max_keywords=self.max_keywords,
            max_methods=self.max_methods,
            max_fields=self.max_fields,
            max_datasets=self.max_datasets,
            max_applications=self.max_applications
        )
        
        try:
            # è°ƒç”¨ LLM
            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æå“åº”
            entities = self._parse_llm_response(response_text)
            return entities
            
        except Exception as e:
            print(f"âŒ å®ä½“æå–å¤±è´¥: {e}")
            return {"keywords": [], "methods": [], "datasets": []}
    
    def extract_from_document(self, document: Document) -> Dict[str, Any]:
        """
        ä»å•ä¸ª LangChain Document ä¸­æå–å®ä½“
        
        Args:
            document: LangChain Document å¯¹è±¡
            
        Returns:
            åŒ…å«æ–‡æ¡£å…ƒä¿¡æ¯å’Œæå–å®ä½“çš„å­—å…¸
        """
        # è·å–æ–‡æ¡£æ¥æºä¿¡æ¯
        source_file = document.metadata.get("source_file", "æœªçŸ¥æ–‡ä»¶")
        page = document.metadata.get("page", 0)
        
        # æå–å®ä½“
        entities = self.extract_entities(document.page_content)
        
        return {
            "source_file": source_file,
            "page": page,
            "entities": entities
        }
    
    def extract_from_documents(
        self,
        documents: List[Document],
        aggregate_by_file: bool = True
    ) -> Dict[str, Dict[str, List[str]]]:
        """
        ä»å¤šä¸ªæ–‡æ¡£ä¸­æ‰¹é‡æå–å®ä½“
        
        Args:
            documents: Document åˆ—è¡¨
            aggregate_by_file: æ˜¯å¦æŒ‰æ–‡ä»¶èšåˆå®ä½“
            
        Returns:
            ä»¥æ–‡ä»¶åä¸º keyï¼Œå®ä½“å­—å…¸ä¸º value çš„å­—å…¸
        """
        if not documents:
            return {}
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„
        file_docs = {}
        for doc in documents:
            source_file = doc.metadata.get("source_file", "æœªçŸ¥æ–‡ä»¶")
            if source_file not in file_docs:
                file_docs[source_file] = []
            file_docs[source_file].append(doc)
        
        results = {}
        
        for source_file, docs in file_docs.items():
            print(f"ğŸ“„ æ­£åœ¨æå–: {source_file}")
            
            if aggregate_by_file:
                # åˆå¹¶åŒä¸€æ–‡ä»¶çš„å‰å‡ é¡µå†…å®¹è¿›è¡Œæå–
                combined_text = "\n\n".join([
                    doc.page_content for doc in docs[:3]  # åªå–å‰3ä¸ªchunk
                ])
                entities = self.extract_entities(combined_text)
            else:
                # åˆ†åˆ«æå–æ¯ä¸ªchunkï¼Œç„¶åå»é‡åˆå¹¶
                all_keywords = set()
                all_methods = set()
                all_datasets = set()
                
                for doc in docs[:5]:  # é™åˆ¶å¤„ç†çš„chunkæ•°é‡
                    result = self.extract_entities(doc.page_content)
                    all_keywords.update(result.get("keywords", []))
                    all_methods.update(result.get("methods", []))
                    all_datasets.update(result.get("datasets", []))
                
                entities = {
                    "keywords": list(all_keywords)[:self.max_keywords],
                    "methods": list(all_methods)[:self.max_methods],
                    "datasets": list(all_datasets)[:self.max_datasets]
                }
            
            results[source_file] = entities
            print(f"  âœ… å…³é”®è¯: {entities['keywords']}")
            print(f"  âœ… æ–¹æ³•: {entities['methods']}")
            print(f"  âœ… æ•°æ®é›†: {entities['datasets']}")
        
        return results


def create_entity_extractor(api_key: Optional[str] = None) -> EntityExtractor:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºå®ä½“æå–å™¨
    
    Args:
        api_key: å¯é€‰çš„ API Key
        
    Returns:
        EntityExtractor å®ä¾‹
    """
    return EntityExtractor(api_key=api_key)

"""
æ–‡æ¡£å¤„ç†æ¨¡å—
è´Ÿè´£ PDF/Word æ–‡æ¡£çš„è§£æå’Œæ–‡æœ¬åˆ‡åˆ†
"""

import os
import re
import tempfile
from pathlib import Path
from typing import List, Optional, BinaryIO

from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import sys
sys.path.append(str(Path(__file__).parent.parent))
from config import CHUNK_SIZE, CHUNK_OVERLAP, UPLOADS_DIR


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼šè§£æå’Œåˆ‡åˆ†æ–‡æ¡£"""
    
    def __init__(
        self,
        chunk_size: int = CHUNK_SIZE,
        chunk_overlap: int = CHUNK_OVERLAP
    ):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap: æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
        # ä½¿ç”¨å¤šç§åˆ†éš”ç¬¦é€’å½’åˆ‡åˆ†ï¼Œä¼˜å…ˆä¿æŒæ®µè½å’Œå¥å­å®Œæ•´æ€§
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=[
                "\n\n",  # æ®µè½åˆ†éš”
                "\n",    # æ¢è¡Œ
                "ã€‚",    # ä¸­æ–‡å¥å·
                "ï¼",    # ä¸­æ–‡æ„Ÿå¹å·
                "ï¼Ÿ",    # ä¸­æ–‡é—®å·
                ".",     # è‹±æ–‡å¥å·
                "!",     # è‹±æ–‡æ„Ÿå¹å·
                "?",     # è‹±æ–‡é—®å·
                ";",     # åˆ†å·
                "ï¼›",    # ä¸­æ–‡åˆ†å·
                " ",     # ç©ºæ ¼
                ""       # å­—ç¬¦çº§åˆ«ï¼ˆæœ€åæ‰‹æ®µï¼‰
            ]
        )
    
    def load_pdf(self, file_path: str) -> List[Document]:
        """
        åŠ è½½ PDF æ–‡ä»¶
        
        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„
            
        Returns:
            Document åˆ—è¡¨ï¼Œæ¯é¡µä¸€ä¸ª Document
        """
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ æ–‡ä»¶åå…ƒæ•°æ®
        file_name = Path(file_path).name
        for doc in documents:
            doc.metadata["source_file"] = file_name
            # é¡µç ä» 1 å¼€å§‹ï¼ˆåŸå§‹æ˜¯ä» 0 å¼€å§‹ï¼‰
            if "page" in doc.metadata:
                doc.metadata["page"] = doc.metadata["page"] + 1
        
        return documents
    
    def load_pdf_from_upload(self, uploaded_file: BinaryIO, filename: str) -> List[Document]:
        """
        ä»ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡åŠ è½½ PDF
        
        Args:
            uploaded_file: Streamlit ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            Document åˆ—è¡¨
        """
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            documents = self.load_pdf(tmp_path)
            # æ›´æ–° source_file ä¸ºåŸå§‹æ–‡ä»¶å
            for doc in documents:
                doc.metadata["source_file"] = filename
            return documents
        finally:
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_path)
    
    def load_word(self, file_path: str) -> List[Document]:
        """
        åŠ è½½ Word æ–‡æ¡£ (.docx)
        
        Args:
            file_path: Word æ–‡ä»¶è·¯å¾„
            
        Returns:
            Document åˆ—è¡¨
        """
        try:
            from docx import Document as DocxDocument
        except ImportError:
            raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")
        
        docx_doc = DocxDocument(file_path)
        file_name = Path(file_path).name
        
        # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬
        full_text = []
        for para in docx_doc.paragraphs:
            if para.text.strip():
                full_text.append(para.text)
        
        # åˆ›å»ºä¸€ä¸ª Document å¯¹è±¡
        content = "\n\n".join(full_text)
        document = Document(
            page_content=content,
            metadata={
                "source_file": file_name,
                "page": 1  # Word æ–‡æ¡£ä¸åˆ†é¡µï¼Œç»Ÿä¸€è®¾ä¸º 1
            }
        )
        
        return [document]
    
    def load_word_from_upload(self, uploaded_file: BinaryIO, filename: str) -> List[Document]:
        """
        ä»ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡åŠ è½½ Word æ–‡æ¡£
        
        Args:
            uploaded_file: Streamlit ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            filename: åŸå§‹æ–‡ä»¶å
            
        Returns:
            Document åˆ—è¡¨
        """
        suffix = ".docx" if filename.endswith(".docx") else ".doc"
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            documents = self.load_word(tmp_path)
            for doc in documents:
                doc.metadata["source_file"] = filename
            return documents
        finally:
            os.unlink(tmp_path)
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬ï¼šç§»é™¤å‚è€ƒæ–‡çŒ®ã€è‡´è°¢ã€é¡µçœ‰é¡µè„šç­‰å™ªéŸ³
        åŒæ—¶å¤„ç† PDF è§£æå¯èƒ½äº§ç”Ÿçš„æ— æ•ˆ Unicode å­—ç¬¦
        """
        # â˜… ç§»é™¤æ— æ•ˆçš„ Unicode ä»£ç†å­—ç¬¦ï¼ˆsurrogatesï¼‰
        try:
            text = text.encode('utf-8', 'surrogatepass').decode('utf-8', 'ignore')
        except Exception:
            text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
        
        # â˜…â˜… ç§»é™¤ PDF è§£æäº§ç”Ÿçš„ Unicode è½¬ä¹‰åºåˆ—åƒåœ¾
        # åŒ¹é… /uniXXXXXXXX æˆ– /uni0000XXXX ç­‰æ¨¡å¼
        text = re.sub(r'/uni[0-9a-fA-F]{8}', '', text)
        text = re.sub(r'/uni[0-9a-fA-F]{4,}', '', text)
        
        # ç§»é™¤å…¶ä»–å¸¸è§çš„ PDF è§£æåƒåœ¾å­—ç¬¦åºåˆ—
        text = re.sub(r'\x00+', '', text)  # NULL å­—ç¬¦
        text = re.sub(r'[\x01-\x08\x0b\x0c\x0e-\x1f]', '', text)  # æ§åˆ¶å­—ç¬¦
        
        # ç§»é™¤è¿ç»­çš„æ•°å­—åƒåœ¾ï¼ˆå¦‚ 00057 å¼€å¤´çš„åºåˆ—ï¼‰
        text = re.sub(r'\b\d{5,}\b(?:/uni[0-9a-fA-F]+)*', '', text)
        
        # ç§»é™¤å¸¸è§çš„é¡µçœ‰é¡µè„šæ¨¡å¼
        text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)
        text = re.sub(r'(?i)page\s*\d+\s*(of\s*\d+)?', '', text)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œå’Œç©ºæ ¼
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {3,}', ' ', text)  # å¤šä¸ªè¿ç»­ç©ºæ ¼
        
        return text.strip()
    
    def is_garbage_chunk(self, text: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªæ–‡æœ¬å—æ˜¯å¦ä¸»è¦æ˜¯åƒåœ¾å†…å®¹
        ä½¿ç”¨é€šç”¨åŒ–è§„åˆ™ï¼Œé€‚ç”¨äºå„ç§è®ºæ–‡æ ¼å¼
        
        Returns:
            True å¦‚æœæ˜¯åƒåœ¾å—åº”è¯¥è¿‡æ»¤æ‰
        """
        # === æ£€æŸ¥ 0ï¼šæœ€å°æœ‰æ•ˆé•¿åº¦ ===
        if len(text) < 100:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ (é•¿åº¦={len(text)}): {preview}...")
            return True
        
        text_lower = text.lower()
        
        # === æ£€æŸ¥ 1ï¼šæœ‰æ„ä¹‰å­—ç¬¦æ¯”ä¾‹ ===
        meaningful_chars = re.findall(r'[a-zA-Z\u4e00-\u9fff0-9\.,;:!?\'"()\[\]\-\s]', text)
        meaningful_ratio = len(meaningful_chars) / len(text) if text else 0
        
        if meaningful_ratio < 0.7:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤åƒåœ¾å— (æœ‰æ„ä¹‰å­—ç¬¦={meaningful_ratio:.1%}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 1.5ï¼šå¤§è§„æ¨¡ä½œè€…åˆ—è¡¨ï¼ˆå¦‚ GPT-4 è®ºæ–‡æœ‰æ•°ç™¾ä½ä½œè€…ï¼‰===
        # ç‰¹å¾ï¼šå¤§é‡é€—å·åˆ†éš”çš„äººåï¼Œå‡ ä¹æ²¡æœ‰åŠ¨è¯æˆ–å¥å­ç»“æ„
        # æ¨¡å¼ï¼š"Name Name, Name Name, Name Name,..." æˆ– "Name, Name, Name,..."
        comma_count = text.count(',')
        if comma_count >= 15:
            # æŒ‰é€—å·åˆ†å‰²ï¼Œè®¡ç®—å¹³å‡æ®µé•¿
            segments = [s.strip() for s in text.split(',') if s.strip()]
            if segments:
                avg_segment_len = sum(len(s) for s in segments) / len(segments)
                # äººåç‰¹å¾ï¼šå¹³å‡æ®µé•¿å¾ˆçŸ­ï¼ˆé€šå¸¸ < 25 å­—ç¬¦ï¼‰
                if avg_segment_len < 25:
                    # è¿›ä¸€æ­¥éªŒè¯ï¼šæ£€æµ‹å¤§å†™å­—æ¯å¼€å¤´çš„å•è¯å¯†åº¦ï¼ˆäººåç‰¹å¾ï¼‰
                    capitalized_words = re.findall(r'\b[A-Z][a-z]+\b', text)
                    words = text.split()
                    cap_ratio = len(capitalized_words) / len(words) if words else 0
                    
                    # å¦‚æœ > 60% çš„å•è¯ä»¥å¤§å†™å¼€å¤´ï¼ˆäººåç‰¹å¾ï¼‰
                    if cap_ratio > 0.6:
                        preview = text[:60].replace('\n', ' ')
                        print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤å¤§è§„æ¨¡ä½œè€…åˆ—è¡¨ (é€—å·={comma_count}, å¹³å‡æ®µé•¿={avg_segment_len:.1f}, å¤§å†™æ¯”={cap_ratio:.1%}): {preview}...")
                        return True
        
        # === æ£€æŸ¥ 2ï¼šå‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼ˆé€šç”¨åŒ–æ£€æµ‹ï¼‰===
        # ç‰¹å¾ï¼šä½œè€…å§“åç¼©å†™(X. Name)ã€å·å·é¡µç (36(4):)ã€å¹´ä»½ã€æœŸåˆŠä¼šè®®å
        ref_indicators = 0
        
        # 2a. ä½œè€…å§“åç¼©å†™æ¨¡å¼ï¼šA. Name, B. Name æˆ– Name, A., Name, B.
        author_initials = re.findall(r'\b[A-Z]\.\s*[A-Z][a-z]+', text)  # A. Smith
        author_initials2 = re.findall(r'[A-Z][a-z]+,\s*[A-Z]\.', text)  # Smith, A.
        if len(author_initials) + len(author_initials2) >= 3:
            ref_indicators += 2
        
        # 2b. å·å·/æœŸå·/é¡µç æ¨¡å¼ï¼š36(4):, vol. 12, pp. 123-456
        volume_patterns = re.findall(r'\d+\(\d+\):', text)  # 36(4):
        page_patterns = re.findall(r'(?:pp?\.|pages?)\s*\d+[-â€“]\d+', text_lower)  # pp. 123-456
        if len(volume_patterns) >= 1 or len(page_patterns) >= 2:
            ref_indicators += 2
        
        # 2c. å¤šä¸ªç‹¬ç«‹å¹´ä»½ï¼ˆå¦‚ ", 2020.", ", 2023."ï¼‰
        year_with_punct = re.findall(r'[,\.]\s*(19|20)\d{2}[,\.\)]', text)
        if len(year_with_punct) >= 3:
            ref_indicators += 1
        
        # 2d. æœŸåˆŠ/ä¼šè®®å…³é”®è¯
        journal_keywords = [
            r'transactions\s+on', r'journal\s+of', r'proceedings\s+of',
            r'conference\s+on', r'symposium\s+on', r'workshop\s+on',
            r'in\s+proc\.', r'arxiv\s*:', r'\bin\s+the\s+\d+'
        ]
        journal_matches = sum(1 for p in journal_keywords if re.search(p, text_lower))
        if journal_matches >= 1:
            ref_indicators += 1
        
        # 2e. è¿ç»­å¤šä¸ªé€—å·åˆ†éš”çš„äººåï¼ˆå‚è€ƒæ–‡çŒ®åˆ—è¡¨ç‰¹å¾ï¼‰
        comma_names = re.findall(r'[A-Z][a-z]+,\s*[A-Z]\.,?\s*(?:and\s+)?[A-Z][a-z]+', text)
        if len(comma_names) >= 2:
            ref_indicators += 1
        
        if ref_indicators >= 3:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤å‚è€ƒæ–‡çŒ®åˆ—è¡¨ (æŒ‡æ ‡={ref_indicators}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 3ï¼šè®ºæ–‡æ ‡é¢˜é¡µ/ä½œè€…ä¿¡æ¯ï¼ˆé€šç”¨åŒ–æ£€æµ‹ï¼‰===
        title_page_indicators = 0
        
        # 3a. ä½œè€…å+æœºæ„ä¸Šæ ‡ï¼šName1, Name2,3, Nameâ€  (æ•°å­—ç´§è·Ÿåå­—)
        author_superscripts = re.findall(r'[A-Z][a-z]+\d+[,\d]*\s', text)
        if len(author_superscripts) >= 2:
            title_page_indicators += 2
        
        # 3b. æœºæ„å+ä¸Šæ ‡æ•°å­—ï¼š1University, 2Google, 3Microsoft
        institution_patterns = re.findall(r'\d+[A-Z][a-z]+\s+(University|Institute|Lab|Google|Microsoft|Meta|Research)', text)
        if len(institution_patterns) >= 1:
            title_page_indicators += 2
        
        # 3c. Abstract å¼€å¤´æ¨¡å¼
        if re.search(r'\babstract\s+(we|how|this|in)\s+', text_lower):
            title_page_indicators += 1
        
        # 3d. è®ºæ–‡æ ‡é¢˜æ ¼å¼ï¼šå¸¦å†’å·çš„æ ‡é¢˜ + æœºæ„
        paper_title_pattern = re.search(r'^[A-Z][^\.]+:\s*[A-Z][^\.]+\s+[A-Z][a-z]+\d', text)
        if paper_title_pattern:
            title_page_indicators += 2
        
        # 3e. é‚®ç®±
        email_count = len(re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', text))
        if email_count >= 1:
            title_page_indicators += 1
        
        if title_page_indicators >= 2:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤æ ‡é¢˜é¡µ/ä½œè€…ä¿¡æ¯ (æŒ‡æ ‡={title_page_indicators}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 4ï¼šå›¾è¡¨æ ‡é¢˜/è¡¨æ ¼æ•°æ®ï¼ˆé€šç”¨åŒ–æ£€æµ‹ï¼‰===
        figure_table_indicators = 0
        
        # 4a. Figure/Table æ ‡é¢˜ï¼ˆä»»æ„ä½ç½®ï¼‰
        fig_table_matches = re.findall(r'(figure|fig\.?|table)\s*\d+\s*[:\.]?', text_lower)
        if len(fig_table_matches) >= 1:
            figure_table_indicators += 1
        # å¤šä¸ªå›¾è¡¨å¼•ç”¨
        if len(fig_table_matches) >= 2:
            figure_table_indicators += 1
        
        # 4b. å­å›¾æ ‡ç­¾ (a), (b), (c) è¿ç»­å‡ºç°
        subfig_labels = re.findall(r'\([a-d]\)', text_lower)
        if len(subfig_labels) >= 3:
            figure_table_indicators += 1
        
        # 4c. è¿ç»­çš„å°æ•°æ•°æ®ï¼ˆè¡¨æ ¼ç‰¹å¾ï¼‰ï¼š0.762 0.833 0.864 æˆ– 92.79 99.89 98.43
        consecutive_decimals = re.findall(r'\d+\.\d+\s+\d+\.\d+\s+\d+\.\d+', text)
        if len(consecutive_decimals) >= 1:
            figure_table_indicators += 2
        
        # 4d. ç©ºæ ¼åˆ†éš”çš„å¤šä¸ªæ•°å­—ï¼ˆè¡¨æ ¼æ•°æ®ï¼‰
        space_separated_nums = re.findall(r'\d+\.\d+\s+\d+\.\d+', text)
        if len(space_separated_nums) >= 3:
            figure_table_indicators += 1
        
        # 4e. Â± ç¬¦å·ï¼ˆè¯¯å·®èŒƒå›´ï¼‰
        if 'Â±' in text and re.search(r'Â±\s*\d+\.?\d*', text):
            figure_table_indicators += 1
        
        # å¦‚æœæœ‰å›¾è¡¨æ ‡é¢˜å’Œæ•°æ®ç‰¹å¾çš„ç»„åˆ
        if figure_table_indicators >= 2:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤å›¾è¡¨/è¡¨æ ¼å†…å®¹ (æŒ‡æ ‡={figure_table_indicators}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 5ï¼šè‡´è°¢/èµ„åŠ©ä¿¡æ¯ ===
        ack_indicators = 0
        
        # 5a. å…³é”®è¯åŒ¹é…
        ack_keywords = [
            r'grant\s*no\.?', r'national\s+science\s+foundation', r'supported\s+by',
            r'funded\s+by', r'this\s+work\s+was\s+supported', r'acknowledgment',
            r'acknowledgement', r'we\s+thank', r'faculty\s+award', r'research\s+award',
            r'nsf\s+', r'onr\s+', r'darpa\s+'
        ]
        ack_matches = sum(1 for p in ack_keywords if re.search(p, text_lower))
        ack_indicators += ack_matches
        
        # 5b. èµ„åŠ©å·æ¨¡å¼ï¼ˆå¦‚ N00014-22-1-2773, IIS-1234567ï¼‰
        grant_numbers = re.findall(r'[A-Z]{2,}\s*[-\s]?\d{4,}[-\d]*', text)
        if len(grant_numbers) >= 1:
            ack_indicators += 2
        
        if ack_indicators >= 2:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤è‡´è°¢/èµ„åŠ©ä¿¡æ¯ (æŒ‡æ ‡={ack_indicators}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 6ï¼šURDF/é…ç½®æ–‡ä»¶å†…å®¹ ===
        urdf_patterns = [
            r'joint_name:', r'joint_type:', r'parent_link:', r'child_link:',
            r'link_\d+', r'joint_\d+', r'<link>', r'<joint>', r'<robot>'
        ]
        urdf_matches = sum(1 for p in urdf_patterns if re.search(p, text_lower))
        if urdf_matches >= 3:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤é…ç½®æ–‡ä»¶å†…å®¹ (URDFåŒ¹é…={urdf_matches}): {preview}...")
            return True
        
        # === æ£€æŸ¥ 7ï¼šé«˜å¯†åº¦å†’å·ï¼ˆkey: value æ ¼å¼ï¼‰===
        colon_count = text.count(':')
        colon_density = colon_count / len(text) * 100 if text else 0
        if colon_density > 3 and colon_count > 10:
            preview = text[:60].replace('\n', ' ')
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤é…ç½®å†…å®¹ (å†’å·å¯†åº¦={colon_density:.1f}%): {preview}...")
            return True
        
        # === æ£€æŸ¥ 8ï¼šé‡å¤æ¨¡å¼ ===
        repeated_pattern = re.findall(r'(\b\w+_\d+\b)', text)
        if len(repeated_pattern) > 10:
            unique_ratio = len(set(repeated_pattern)) / len(repeated_pattern)
            if unique_ratio < 0.3:
                preview = text[:60].replace('\n', ' ')
                print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤é‡å¤æ¨¡å¼ (å”¯ä¸€ç‡={unique_ratio:.1%}): {preview}...")
                return True
        
        return False

    def remove_references_section(self, documents: List[Document]) -> List[Document]:
        """
        ã€å¼ºåŠ›æˆªæ–­ã€‘å®è§‚è¿‡æ»¤ï¼š
        æ‰«ææ‰€æœ‰é¡µé¢ï¼Œä¸€æ—¦æ£€æµ‹åˆ° 'References'ã€'Bibliography' æˆ– 'å‚è€ƒæ–‡çŒ®' ç‹¬ç«‹æ ‡é¢˜ï¼Œ
        ç›´æ¥ä¸¢å¼ƒè¯¥é¡µåŠå…¶ä¹‹åçš„æ‰€æœ‰é¡µé¢ã€‚
        
        ç­–ç•¥ï¼šå®å¯é”™æ€ä¸å¯æ”¾è¿‡ï¼Œå› ä¸ºå‚è€ƒæ–‡çŒ®é€šå¸¸åœ¨æ–‡æœ«ã€‚
        """
        cutoff_index = -1
        
        # å‚è€ƒæ–‡çŒ®æ ‡é¢˜çš„æ­£åˆ™æ¨¡å¼åˆ—è¡¨ï¼ˆè¡Œé¦–åŒ¹é…ï¼‰
        # æ”¯æŒå¤šç§æ ¼å¼ï¼šçº¯æ ‡é¢˜ã€å¸¦ç¼–å·ï¼ˆ6. References, VI. Referencesï¼‰ã€ä¸­æ–‡
        reference_patterns = [
            # çº¯æ ‡é¢˜ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
            r'^references\s*$',
            r'^bibliography\s*$',
            r'^reference\s*$',
            r'^å‚è€ƒæ–‡çŒ®\s*$',
            r'^å¼•ç”¨æ–‡çŒ®\s*$',
            # å¸¦é˜¿æ‹‰ä¼¯æ•°å­—ç¼–å·ï¼ˆå¦‚ "6. References", "7 References"ï¼‰
            r'^\d+\.?\s+references\s*$',
            r'^\d+\.?\s+bibliography\s*$',
            r'^\d+\.?\s*å‚è€ƒæ–‡çŒ®\s*$',
            # å¸¦ç½—é©¬æ•°å­—ç¼–å·ï¼ˆå¦‚ "VI. References", "V References"ï¼‰
            r'^[ivxIVX]+\.?\s+references\s*$',
            r'^[ivxIVX]+\.?\s+bibliography\s*$',
            # å¸¦æ–¹æ‹¬å·ç¼–å·ï¼ˆå¦‚ "[6] References"ï¼‰
            r'^\[\d+\]\s*references\s*$',
        ]
        
        # ä»å¤´å¼€å§‹æ‰«ææ‰€æœ‰é¡µé¢ï¼ˆä¸é™åˆ¶èŒƒå›´ï¼Œç¡®ä¿ä¸é—æ¼ï¼‰
        for i, doc in enumerate(documents):
            content = doc.page_content
            lines = content.split('\n')
            
            # æ£€æŸ¥é¡µé¢çš„å‰ 10 è¡Œï¼ˆæ ‡é¢˜é€šå¸¸åœ¨é¡µé¢é¡¶éƒ¨ï¼‰
            for line in lines[:10]:
                clean_line = line.strip().lower()
                
                # è·³è¿‡ç©ºè¡Œ
                if not clean_line:
                    continue
                
                # å°è¯•åŒ¹é…æ‰€æœ‰æ¨¡å¼
                for pattern in reference_patterns:
                    if re.match(pattern, clean_line, re.IGNORECASE):
                        cutoff_index = i
                        page_num = doc.metadata.get('page', i + 1)
                        print(f"[DocumentProcessor] âš ï¸ æ£€æµ‹åˆ°å‚è€ƒæ–‡çŒ®æ ‡é¢˜äºç¬¬ {page_num} é¡µ: '{line.strip()}'")
                        print(f"[DocumentProcessor] ğŸ”ª å¼ºåŠ›æˆªæ–­ï¼šä¸¢å¼ƒç¬¬ {page_num} é¡µåŠä¹‹åå…± {len(documents) - i} é¡µ")
                        break
                
                if cutoff_index != -1:
                    break
            
            if cutoff_index != -1:
                break
        
        # å¦‚æœæ‰¾åˆ°äº†å‚è€ƒæ–‡çŒ®é¡µï¼Œæˆªæ–­è¯¥é¡µåŠå…¶ä¹‹åçš„æ‰€æœ‰é¡µé¢
        if cutoff_index != -1:
            return documents[:cutoff_index]
        
        return documents
    
    def is_reference_chunk(self, text: str) -> bool:
        """
        ã€è¶…å¼ºè¿‡æ»¤ç‰ˆã€‘å¾®è§‚è¿‡æ»¤ï¼š
        åˆ¤æ–­ä¸€ä¸ªæ–‡æœ¬å—æ˜¯å¦ä¸»è¦æ˜¯å‚è€ƒæ–‡çŒ®å†…å®¹ã€‚
        ä½¿ç”¨å¤šç§ç‰¹å¾ç»¼åˆåˆ¤æ–­ï¼Œå®å¯é”™æ€ä¸å¯æ”¾è¿‡ã€‚
        """
        text_length = len(text)
        if text_length < 50:
            return False  # å¤ªçŸ­çš„ä¸å¤„ç†
        
        # === ç‰¹å¾ 1ï¼šå¼•ç”¨æ ‡è®°æ¨¡å¼ [1], [23], [1,2,3] ===
        citation_pattern = r'\[\d+(?:,\s*\d+)*\]'
        citations = re.findall(citation_pattern, text)
        
        # === ç‰¹å¾ 2ï¼šè¿ç»­ç¼–å·å¼•ç”¨æ£€æµ‹ï¼ˆå¦‚ [1] xxx [2] xxx [3] xxxï¼‰===
        sequential_citations = re.findall(r'\[(\d+)\]', text)
        has_sequential = False
        if len(sequential_citations) >= 3:
            nums = [int(n) for n in sequential_citations[:10]]
            for i in range(len(nums) - 2):
                if nums[i+1] == nums[i] + 1 and nums[i+2] == nums[i] + 2:
                    has_sequential = True
                    break
        
        # === ç‰¹å¾ 3ï¼šä½œè€…+å¹´ä»½æ ¼å¼ï¼ˆå¦‚ "Smith et al., 2022" æˆ– "Name (2021)"ï¼‰===
        # è¿™æ˜¯å‚è€ƒæ–‡çŒ®åˆ—è¡¨çš„å¼ºç‰¹å¾
        author_year_pattern = r'(?:[A-Z][a-z]+\s+(?:et\s+al\.?|and\s+[A-Z][a-z]+)?\s*[\(\,]\s*(?:19|20)\d{2}\s*[\)\,])'
        author_year_matches = re.findall(author_year_pattern, text)
        
        # === ç‰¹å¾ 4ï¼šarXiv å¼•ç”¨æ¨¡å¼ ===
        arxiv_refs = re.findall(r'arXiv', text, re.IGNORECASE)
        
        # === ç‰¹å¾ 5ï¼šå¹´ä»½æ¨¡å¼ (å¦‚ 2023, 2021) ===
        year_pattern = r'\b(19|20)\d{2}\b'
        years = re.findall(year_pattern, text)
        
        # === ç‰¹å¾ 6ï¼šä¼šè®®/æœŸåˆŠå…³é”®è¯ ===
        venue_keywords = [
            'IEEE', 'ACM', 'CVPR', 'ICCV', 'ECCV', 'NeurIPS', 'ICML', 'ICLR',
            'AAAI', 'IJCAI', 'preprint', 'Proceedings', 'Conference', 'Journal',
            'Transactions', 'vol.', 'pp.', 'eds.', 'et al.', 'In Proceedings',
            'Workshop', 'Symposium', 'Annual Meeting', 'arXiv', 'abs/'
        ]
        venue_count = sum(1 for kw in venue_keywords if kw.lower() in text.lower())
        
        # === ç‰¹å¾ 7ï¼šé¡µç æ¨¡å¼ï¼ˆå¦‚ pp. 123-456, pages 770â€“778ï¼‰===
        page_patterns = re.findall(r'(?:pp\.|pages?)\s*\d+[\-â€“]\d+', text, re.IGNORECASE)
        
        # === ç‰¹å¾ 8ï¼šDOI/URL å¯†åº¦ ===
        doi_count = len(re.findall(r'doi[:\.\s]', text, re.IGNORECASE))
        url_count = len(re.findall(r'https?://', text))
        
        # === ç»¼åˆåˆ¤æ–­é€»è¾‘ï¼ˆæ›´æ¿€è¿›çš„é˜ˆå€¼ï¼‰ ===
        is_ref = False
        reason = ""
        
        # è§„åˆ™ 1ï¼šæ£€æµ‹åˆ°è¿ç»­ç¼–å·å¼•ç”¨ï¼ˆå¼ºç‰¹å¾ï¼‰
        if has_sequential:
            is_ref = True
            reason = f"è¿ç»­ç¼–å·å¼•ç”¨ {sequential_citations[:5]}"
        
        # è§„åˆ™ 2ï¼šä½œè€…+å¹´ä»½æ ¼å¼ >= 2 ä¸ª
        elif len(author_year_matches) >= 2:
            is_ref = True
            reason = f"ä½œè€…å¹´ä»½æ ¼å¼={len(author_year_matches)}"
        
        # è§„åˆ™ 3ï¼šå¼•ç”¨æ ‡è®° >= 2 ä¸ªï¼ˆé™ä½é˜ˆå€¼ï¼‰
        elif len(citations) >= 2:
            is_ref = True
            reason = f"å¼•ç”¨æ ‡è®°æ•°={len(citations)}"
        
        # è§„åˆ™ 4ï¼šarXiv å¼•ç”¨ >= 1 ä¸ªï¼ˆé™ä½é˜ˆå€¼ï¼‰
        elif len(arxiv_refs) >= 1:
            is_ref = True
            reason = f"arXivå¼•ç”¨æ•°={len(arxiv_refs)}"
        
        # è§„åˆ™ 5ï¼šå¹´ä»½ + ä¼šè®®ç»„åˆï¼ˆé™ä½é˜ˆå€¼ï¼‰
        elif len(years) >= 3 and venue_count >= 1:
            is_ref = True
            reason = f"å¹´ä»½æ•°={len(years)}, ä¼šè®®è¯={venue_count}"
        
        # è§„åˆ™ 6ï¼šé¡µç æ¨¡å¼
        elif len(page_patterns) >= 1:
            is_ref = True
            reason = f"é¡µç æ¨¡å¼={page_patterns}"
        
        # è§„åˆ™ 7ï¼šé«˜å¯†åº¦ DOI/URL
        elif doi_count >= 1 or url_count >= 2:
            is_ref = True
            reason = f"DOIæ•°={doi_count}, URLæ•°={url_count}"
        
        # è§„åˆ™ 8ï¼šç»¼åˆå¯†åº¦åˆ¤æ–­ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        else:
            density_score = (
                len(citations) * 2 +
                len(arxiv_refs) * 3 +
                len(author_year_matches) * 2 +
                len(years) * 0.3 +
                venue_count * 1.5 +
                len(page_patterns) * 2 +
                doi_count * 2 +
                url_count
            ) / (text_length / 100)
            
            if density_score > 1.5:  # é™ä½é˜ˆå€¼
                is_ref = True
                reason = f"ç»¼åˆå¯†åº¦={density_score:.2f}"
        
        # è°ƒè¯•è¾“å‡º
        if is_ref:
            preview = text[:80].replace('\n', ' ') + '...'
            print(f"[DocumentProcessor] ğŸ—‘ï¸ è¿‡æ»¤å‚è€ƒæ–‡çŒ®å— ({reason}): {preview}")
        
        return is_ref
    
    def split_documents(
        self,
        documents: List[Document],
        clean: bool = True
    ) -> List[Document]:
        """
        åˆ‡åˆ†æ–‡æ¡£ä¸ºå°å—
        
        Args:
            documents: åŸå§‹ Document åˆ—è¡¨
            clean: æ˜¯å¦å…ˆæ¸…æ´—æ–‡æœ¬
            
        Returns:
            åˆ‡åˆ†åçš„ Document åˆ—è¡¨ï¼ˆchunksï¼‰
        """
        if clean:
            # 1. ã€å®è§‚æˆªæ–­ã€‘å…ˆå°è¯•å»æ‰æ•´ä¸ªå‚è€ƒæ–‡çŒ®ç« èŠ‚
            documents = self.remove_references_section(documents)
            
            # 2. æ¸…æ´—æ¯ä¸ªæ–‡æ¡£çš„æ–‡æœ¬
            for doc in documents:
                doc.page_content = self.clean_text(doc.page_content)
        
        # ä½¿ç”¨ text_splitter åˆ‡åˆ†
        chunks = self.text_splitter.split_documents(documents)
        
        # 3. ã€å¾®è§‚è¿‡æ»¤ã€‘è¿‡æ»¤æ‰æ®‹ç•™çš„å‚è€ƒæ–‡çŒ®å†…å®¹å’Œåƒåœ¾å—
        filtered_chunks = []
        ref_removed = 0
        garbage_removed = 0
        
        for chunk in chunks:
            content = chunk.page_content
            
            # è¿‡æ»¤åƒåœ¾å—ï¼ˆUnicode è½¬ä¹‰åºåˆ—ç­‰ï¼‰
            if clean and self.is_garbage_chunk(content):
                garbage_removed += 1
                continue
            
            # è¿‡æ»¤å‚è€ƒæ–‡çŒ®å—
            if clean and self.is_reference_chunk(content):
                ref_removed += 1
                continue
            
            filtered_chunks.append(chunk)
        
        if ref_removed > 0 or garbage_removed > 0:
            print(f"[DocumentProcessor] âœ… è¿‡æ»¤å®Œæˆ: å‚è€ƒæ–‡çŒ®å—={ref_removed}, åƒåœ¾å—={garbage_removed}, ä¿ç•™={len(filtered_chunks)}")
        
        # ä¸ºæ¯ä¸ª chunk æ·»åŠ ç´¢å¼•
        for i, chunk in enumerate(filtered_chunks):
            chunk.metadata["chunk_index"] = i
        
        return filtered_chunks
    
    def process_uploaded_file(
        self,
        uploaded_file: BinaryIO,
        filename: str,
        clean: bool = True
    ) -> List[Document]:
        """
        å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šåŠ è½½ + åˆ‡åˆ†
        
        Args:
            uploaded_file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            filename: æ–‡ä»¶å
            clean: æ˜¯å¦æ¸…æ´—æ–‡æœ¬
            
        Returns:
            åˆ‡åˆ†åçš„ Document åˆ—è¡¨
        """
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½æ–¹æ³•
        if filename.lower().endswith(".pdf"):
            documents = self.load_pdf_from_upload(uploaded_file, filename)
        elif filename.lower().endswith((".docx", ".doc")):
            documents = self.load_word_from_upload(uploaded_file, filename)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
        
        # åˆ‡åˆ†æ–‡æ¡£
        chunks = self.split_documents(documents, clean=clean)
        
        return chunks